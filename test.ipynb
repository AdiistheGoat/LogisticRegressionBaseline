{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2a5228",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f4b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from collections import defaultdict\n",
    "import pickle as pl\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d81057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ab83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e98a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('papermodeldata(in).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba64a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns) -5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ttd'].isnull().any()\n",
    "df['fold'].isnull().any()\n",
    "df['time'].isnull().any()\n",
    "df['y'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8fedcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_labels = df['y'].groupby(df['y']).count()\n",
    "count_labels[1]/count_labels[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89199aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VitalID'].groupby(df['VitalID']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b54b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dead = df[df['y'] == 1]\n",
    "df_alive = df[df['y'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alive['VitalID'].groupby(df_alive['VitalID']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dead['VitalID'].groupby(df_dead['VitalID']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_folds = df.groupby(['fold','y']).size()\n",
    "first = None\n",
    "second = None\n",
    "count = 0\n",
    "for i in count_folds:\n",
    "    count+=1\n",
    "    if not(first):\n",
    "        first = i\n",
    "    else:\n",
    "        second = i\n",
    "\n",
    "    if(count==2):\n",
    "        count = 0\n",
    "        print((second/first)*100)\n",
    "        first = None \n",
    "        second = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29727d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df.drop(['VitalID', 'time','fold','ttd'], axis=1)\n",
    "df_na_dropped = df_dropped.dropna()\n",
    "df_dead = df_dropped[df_dropped['y'] == 1]\n",
    "df_alive = df_dropped[df_dropped['y'] == 0]\n",
    "df_dead_na_dropped = df_na_dropped[df_na_dropped['y'] == 1]\n",
    "df_alive_na_dropped = df_na_dropped[df_na_dropped['y'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba51166",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((len(df_dead)- len(df_dead_na_dropped)) /len(df_dead)) * 100)\n",
    "print(((len(df_alive) - len(df_alive_na_dropped)) /len(df_alive))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde131e2",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the y seprate \n",
    "# remove nan values \n",
    "# remove duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ttd is there , that means the baby is eventually going to die..\n",
    "# if ttd<=days , put y = 1\n",
    "# if ttd>days  , put y =0\n",
    "def format_y(no_of_days,ttd,y_values):\n",
    "    for index in range(len(ttd)):\n",
    "        if(ttd[index]<=no_of_days):\n",
    "            y_values[index] = 1\n",
    "        else:\n",
    "            y_values[index] = 0\n",
    "    return y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33871e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "def winsorize_col(column):\n",
    "    return winsorize(column, limits=(0.001, .001))\n",
    "\n",
    "def winsorize_df(df):\n",
    "    return df.apply(winsorize_col,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96072373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(df,days):\n",
    "    ttd = df['ttd']\n",
    "    y_arr = df['y']\n",
    "    y_arr = format_y(days,ttd,y_arr)\n",
    "    df['y'] = y_arr\n",
    "    # change the y column in the df to the y_arr\n",
    "\n",
    "    # we only care if the trainign data colun values contain NaN values \n",
    "    df_drop_col = df.drop(['VitalID','ttd','time'], axis=1)\n",
    "    df_drop_col_dup = df_drop_col.drop_duplicates()\n",
    "    df_drop_col_dup_na = df_drop_col_dup.dropna()\n",
    "\n",
    "    y_arr = df_drop_col_dup_na['y']\n",
    "    fold_arr = df_drop_col_dup_na['fold']\n",
    "\n",
    "    df_drop_col_dup_na.drop(['y','fold'], axis=1,inplace=True)\n",
    "    df = winsorize_df(df_drop_col_dup_na)\n",
    "    return df,fold_arr,y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f693e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Basics = [ \"VitalID\",\"time\",\"fold\",\"ttd\",\"y\"]\n",
    "\n",
    "Demographics = ['bwt', 'ega', 'male', 'apgar5']\n",
    "\n",
    "HR = ['HR.SB.MotifTwo.diff.uu', 'HR.SB.MotifTwo.diff.uuu','HR.ST.LocalExtrema.l25.diffmaxabsmin', 'HR.std','HR.SB.MotifThree.diffquant.hhhh',\n",
    "       'HR.EX.MovingThreshold.a0.25.b0.1.meanqover', 'HR.DN.cv.3','HR.SB.MotifTwo.iqr.ddd','HR.SY.StdNthDer.5','HR.ST.LocalExtrema.l25.maxmaxmed', 'HR.SY.StdNthDer.17', 'HR.Quantile.99', 'HR.FC.Suprise.tstat',\n",
    "       'HR.SB.MotifThree.quantile.hhhh','HR.EX.MovingThreshold.a0.25.b0.05.meanqover','HR.PH.Walkerrunningvar...sw.meanabsdiff', 'HR.CO.tc3.1..denom', 'HR.mean',\n",
    "       'HR.skew2','HR.SB.TransitionMatrix23.sumdiagcov','HR.ST.LocalExtrema.n100.minabsmin', 'HR.kurt2', 'HR.MF.arfit.sbc.7'\n",
    "       ]\n",
    "\n",
    "SPO2 = [ 'SP.EX.MovingThreshold.a0.25.b0.1.meanqover', 'SP.EX.MovingThreshold.a1.b0.25.iqrq', 'SP.PH.Walkerbiasprop.0.1..0.5..sw.meanabsdiff',\n",
    "'SP.EX.MovingThreshold.a0.25.b0.1.maxq', 'SP.PH.Walkerprop.0.9..w.std', 'SP.skew2','SP.SB.MotifTwo.diff.dduu','SP.DN.RemovePointsmin.0.2.mean',\n",
    "'SP.PH.Walkermomentum.2..sw.stdrat','SP.CO.tc3.1..denom','SP.mean','SP.kurt2', 'SP.SB.MotifThree.diffquant.hhh','SP.SB.MotifTwo.mean.dddd',\n",
    "'SP.AutoCorr.lag.4','SP.SB.TransitionMatrix41.ondiag', 'SP.SB.MotifThree.quantile.baaa','SP.std', 'SP.SB.TransitionMatrix21.T10', 'SP.SB.BinaryMethod.iqr.pstretch1'\n",
    ",'SP.PH.Walkerprop.0.9..sw.stdrat','SP.MF.arfit.sbc.7','SP.SB.TransitionMatrix22.mineig','SP.ST.LocalExtrema.n100.minabsmin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a6b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diff_dfs(df):\n",
    "    df_hr = df[Basics + HR]\n",
    "    df_spo2 = df[Basics + SPO2]\n",
    "    df_demographics = df[Basics + Demographics]\n",
    "    df_hr_spo2 = df[Basics + HR + SPO2] \n",
    "    df_hr_spo2_demographics = df[Basics + HR + SPO2 + Demographics]\n",
    "    return [df_hr,df_spo2,df_demographics,df_hr_spo2,df_hr_spo2_demographics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[df_hr,df_spo2,df_demographics,df_hr_spo2,df_hr_spo2_demographics] = create_diff_dfs(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39e05dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fbc84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while (curr_no_of_features(variable) >= 5)\n",
    "    # for the different no of folds(variable)\n",
    "        # take only that fold dataset from the df(variable) and its corresponding y for validation \n",
    "        # the rest of the data will be used to train the model\n",
    "\n",
    "        # fit the model \n",
    "        # do the inference on the hold out set \n",
    "        # store the cv accuracy score \n",
    "\n",
    "        # call the permuation_importance function and provide the fitted model and hold out set as arguments \n",
    "        # store the feature importance of all the features \n",
    "\n",
    "    # average out the feature importance across the folds and get the argmin \n",
    "    # store the no of features \n",
    "    # store the average of the cv scores\n",
    "    # store the standard deviation of the cv scores\n",
    "\n",
    "    # no_of_features to decrease = min(5,curr_no_features-5)\n",
    "    # if(no_of_features to decrease):\n",
    "    #      break\n",
    "    # remove the features from the dataset and continue with the changed dataset \n",
    "\n",
    "# Arguments: df(the dataset features), fold_arr, y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256c740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_precision_recall_curve(y_true,y_pred,features_str_name,cv_split_no,no_of_days):\n",
    "    display = PrecisionRecallDisplay.from_predictions(y_true,y_pred)\n",
    "\n",
    "    output_dir = '/Users/adityagoyal/Desktop/Research - yin li/baseline/images'\n",
    "    filename = f\"{features_str_name}-{no_of_days}-{cv_split_no}\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save the figure to the specified directory\n",
    "    display.plot()\n",
    "    plt.savefig(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_arr = sorted(np.random.rand(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_results(df,fold_arr: list,y_arr: list,decrement: int,lower_bound: int,max_iters: int):\n",
    "    results = defaultdict(list)\n",
    "    folds = sorted(fold_arr.unique())\n",
    "    curr_features = list(df.columns)\n",
    "\n",
    "    while len(curr_features) >= lower_bound:\n",
    "        cv_scores = []\n",
    "        feature_importances = []\n",
    "        roc_auc_scores = []\n",
    "        auprc_scores = []\n",
    "\n",
    "        precisions_arr = []\n",
    "        recalls_arr = []\n",
    "        false_positive_rates_arr = []\n",
    "\n",
    "        print(\"Current features: \" + str(len(curr_features)))\n",
    "\n",
    "        for fold_val in folds:\n",
    "\n",
    "            # Split data\n",
    "            X_val = df[fold_arr == fold_val][curr_features].to_numpy()\n",
    "            y_val = y_arr[fold_arr == fold_val].to_numpy()\n",
    "\n",
    "            X_train = df[fold_arr != fold_val][curr_features].to_numpy()\n",
    "            y_train = y_arr[fold_arr != fold_val].to_numpy()\n",
    "\n",
    "            # Fit model\n",
    "            model = LogisticRegression(max_iter=max_iters).fit(X_train, y_train)\n",
    "\n",
    "            # get the test out set scores\n",
    "            y_pred = model.predict(X_val)\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            cv_scores.append(acc)\n",
    "\n",
    "            # Permutation importance\n",
    "            perm = permutation_importance(model, X_val, y_val, n_repeats=10, random_state=42)\n",
    "            feature_importances.append(perm.importances_mean)\n",
    "\n",
    "            # Get the probability estimates for the positive class\n",
    "            y_scores = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "            roc_auc_scores.append(roc_auc_score(y_val,y_scores))\n",
    "            auprc_scores.append(average_precision_score(y_val, y_scores))\n",
    "\n",
    "            recall_arr = []\n",
    "            precision_arr = []\n",
    "            false_positive_rate_arr = []\n",
    "            y_val = np.array(y_val)\n",
    "\n",
    "            for logit in logits_arr:\n",
    "\n",
    "                y_scores_pred = np.array(y_scores) > logit\n",
    "                true_positives = sum(np.logical_and(y_scores_pred==1,y_val==1))\n",
    "                false_negatives = sum(np.logical_and(y_scores_pred == 0, y_val==1))\n",
    "                false_positives = sum(np.logical_and(y_scores_pred == 1, y_val==0))\n",
    "                true_negatives = sum(np.logical_and(y_scores_pred==0,y_val==0))\n",
    "\n",
    "                recall = true_positives/(true_positives+false_negatives)\n",
    "                precision = true_positives/(true_positives+false_positives)\n",
    "                fpr = false_positives/(false_positives+true_negatives)\n",
    "\n",
    "                if(math.isnan(precision)):\n",
    "                    precision_arr.append(1)\n",
    "                else:\n",
    "                    precision_arr.append(precision)\n",
    "\n",
    "                recall_arr.append(recall)\n",
    "                false_positive_rate_arr.append(fpr)\n",
    "\n",
    "            recalls_arr.append(recall_arr)\n",
    "            precisions_arr.append(precision_arr)\n",
    "            false_positive_rates_arr.append(false_positive_rate_arr)\n",
    "\n",
    "\n",
    "        # Store results\n",
    "        results['num_features'].append(len(curr_features))\n",
    "        results['cv_score_mean'].append(np.mean(cv_scores))\n",
    "        results['cv_score_std'].append(np.std(cv_scores))\n",
    "        results['roc_auc_score'].append(np.mean(roc_auc_scores))\n",
    "        results['aurpc_score'].append(np.mean(auprc_scores))\n",
    "\n",
    "        recalls_arr_mean = np.mean(recalls_arr,axis =0)  # true postive rate = recall\n",
    "        precisions_arr_mean = np.mean(precisions_arr,axis =0)\n",
    "        false_positive_rates_arr_mean = np.mean(false_positive_rates_arr,axis=0)\n",
    "\n",
    "        recall_arr_std = np.std(recalls_arr,axis=0)\n",
    "        precision_arr_std = np.std(precisions_arr,axis=0)\n",
    "        false_positive_rate_arr_std = np.std(false_positive_rates_arr,axis=0)\n",
    "\n",
    "        results['recalls_arr_mean'].append(recalls_arr_mean)\n",
    "        results['precisions_arr_mean'].append(precisions_arr_mean)\n",
    "        results['false_positive_rates_arr_mean'].append(false_positive_rates_arr_mean)\n",
    "        results['recall_arr_std'].append(recall_arr_std)\n",
    "        results['precision_arr_std'].append(precision_arr_std)\n",
    "        results['false_positive_rate_arr_std'].append(false_positive_rate_arr_std)\n",
    "\n",
    "\n",
    "        # Average feature importance across folds\n",
    "        avg_importance = np.mean(feature_importances, axis=0)\n",
    "        # print(avg_importance)\n",
    "\n",
    "        # Find least important features\n",
    "        num_to_remove = min(decrement, len(curr_features) - lower_bound)\n",
    "        \n",
    "        if num_to_remove == 0:\n",
    "            break\n",
    "\n",
    "        remove_idx = np.argsort(avg_importance)[:num_to_remove]\n",
    "        remove_features = [curr_features[i] for i in remove_idx]\n",
    "\n",
    "        # Remove features for next iteration\n",
    "        # works on feature names....have to ensure feature names are different\n",
    "        curr_features = [f for f in curr_features if f not in remove_features]\n",
    "\n",
    "    # Convert results to DataFrame for inspection\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28962465",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [1,3,7]\n",
    "dfs_arr = create_diff_dfs(df)\n",
    "results_dict = defaultdict(list)\n",
    "no_of_model_features = [5,5,4,5,6] # specifying the lower bound\n",
    "decrement = 5  # specifying the no of features to decrement by every time\n",
    "\n",
    "for day in days: \n",
    "    for dataframe_index in range(len(dfs_arr)):\n",
    "        data,fold_arr,y_arr = create_df(dfs_arr[dataframe_index],day)\n",
    "        results = give_results(data,fold_arr,y_arr,decrement,no_of_model_features[dataframe_index],2000)\n",
    "        results_dict[day].append(results)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef64e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.pkl','wb') as file:\n",
    "    # Dump data with highest protocol for best performance\n",
    "    pl.dump(results_dict, file, protocol=pl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85827a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results.pkl', 'rb') as f:\n",
    "        # Load the data from the pickle file\n",
    "        loaded_data = pl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e4c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03407ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dir for results \n",
    "for day in days: \n",
    "    # create dir for day\n",
    "    for dataframe_index in range(len(dfs_arr)):\n",
    "        # create dir for dataframe_index..this is the last dir\n",
    "        result = loaded_data[day][dataframe_index]\n",
    "\n",
    "        x_precision = result['precisions_arr_mean']\n",
    "        x_recall = result['recalls_arr_mean']\n",
    "        x_fpr = result['false_positive_rates_arr_mean']\n",
    "        x_precision_std = result['precision_arr_std']\n",
    "        x_recall_std = result['recall_arr_std']\n",
    "        x_fpr_std = result['false_positive_rate_arr_std']\n",
    "\n",
    "        for entry in x_precision:\n",
    "            pass\n",
    "\n",
    "        for entry in x_recall:\n",
    "            pass\n",
    "\n",
    "        for entry in x_fpr:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision is basically out of all the datapoints you predicted as a class, how much of that is correct.\n",
    "# Recall is basically, out of all the datapoints belonging to that class, how much were you able to predict correctly.\n",
    "# You can get this intuition for either of the classes, especially for the imbalanced class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d52b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08106467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive and negative are the two classes\n",
    "# The true positive rate is a measure of the probability that an actual positive instance will be classified as positive.\n",
    "# The true negative rate is a measure of the probability that an actual negative instance will be classified as negative. \n",
    "# he false positive rate is essentially a measure of how often a \"false alarm\" will occur â€“ or, how often an actual negative instance will be classified as positive. (actually this metric says inofrmation aboput the other class!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b1134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision = tp/(tp+fp)\n",
    "# recall = tp/(tp + fn)  (also known as the true positive rate)\n",
    "# f1 = 2*precision*recall/ (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we talk about metrics we talk about only pertaining to one class\n",
    "# true positive rate = tp/(tp + fn) \n",
    "# false positive rate = fp(falsely predicted negative datapoints)/fp+tn (all the negative datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eac931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your use case determines how much importance you give to partiuclar evaluation metrics and which evluation metrics you use and on which class - the 0 or the 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8951919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each fold(hold out set): \n",
    "    # Get the predicted logits\n",
    "    # figure out the different values of the boundary logits (from 0 to 1) say k (like randomly generate unformly spaced values)\n",
    "    # get the precision recall values for each of the boundary logits \n",
    "\n",
    "# average out the precision scores at a particualr boundary logit across the various folds \n",
    "# average out the recall scores at a particualr boundary logit across the various folds \n",
    "# take the stadnard deviation of the precision socres at a particualr boundary logit across the various folds \n",
    "# take the stadnard deviation of the recall socres at a particualr boundary logit across the various folds \n",
    "\n",
    "# shape of the precision scores: (folds,k)\n",
    "# shape of the recall curves: (folds,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each fold(hold out set): \n",
    "    # Get the predicted logits\n",
    "    # figure out the different values of the boundary logits (from 0 to 1) say k (like randomly generate unformly spaced values)\n",
    "    # get the tpr and fpr values for each of the boundary logits \n",
    "\n",
    "# average out the tpr socres at a particualr boundary logit across the various folds \n",
    "# average out the fpr scores at a particualr boundary logit across the various folds \n",
    "# take the stadnard deviation of the tpr socres at a particualr boundary logit across the various folds \n",
    "# take the stadnard deviation of the fpr socres at a particualr boundary logit across the various folds \n",
    "\n",
    "# shape of the tpr scores: (folds,k)\n",
    "# shape of the fpr curves: (folds,k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
