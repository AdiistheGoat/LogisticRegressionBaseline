{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2a5228",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f4b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from collections import defaultdict\n",
    "import pickle as pl\n",
    "import math\n",
    "from sklearn.metrics import (precision_recall_curve,\n",
    "                             PrecisionRecallDisplay)\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from ordered_set import OrderedSet\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e98a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('papermodeldata(in).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba64a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns) -5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ttd'].isnull().any()\n",
    "df['fold'].isnull().any()\n",
    "df['time'].isnull().any()\n",
    "df['y'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8fedcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_labels = df['y'].groupby(df['y']).count()\n",
    "count_labels[1]/count_labels[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89199aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VitalID'].groupby(df['VitalID']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7b54b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dead = df[df['y'] == 1]\n",
    "df_alive = df[df['y'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alive['VitalID'].groupby(df_alive['VitalID']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc5724",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dead['VitalID'].groupby(df_dead['VitalID']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_folds = df.groupby(['fold','y']).size()\n",
    "first = None\n",
    "second = None\n",
    "count = 0\n",
    "for i in count_folds:\n",
    "    count+=1\n",
    "    if not(first):\n",
    "        first = i\n",
    "    else:\n",
    "        second = i\n",
    "\n",
    "    if(count==2):\n",
    "        count = 0\n",
    "        print((second/first)*100)\n",
    "        first = None \n",
    "        second = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29727d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df.drop(['VitalID', 'time','fold','ttd'], axis=1)\n",
    "df_na_dropped = df_dropped.dropna()\n",
    "df_dead = df_dropped[df_dropped['y'] == 1]\n",
    "df_alive = df_dropped[df_dropped['y'] == 0]\n",
    "df_dead_na_dropped = df_na_dropped[df_na_dropped['y'] == 1]\n",
    "df_alive_na_dropped = df_na_dropped[df_na_dropped['y'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba51166",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((len(df_dead)- len(df_dead_na_dropped)) /len(df_dead)) * 100)\n",
    "print(((len(df_alive) - len(df_alive_na_dropped)) /len(df_alive))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde131e2",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the y seprate \n",
    "# remove nan values \n",
    "# remove duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ttd is there , that means the baby is eventually going to die..\n",
    "# if ttd<=days , put y = 1\n",
    "# if ttd>days  , put y =0\n",
    "def format_y(ttd,y_values,lowerbound,upperbound):\n",
    "    \"\"\"\n",
    "    Assigns binary labels to y_values based on ttd and day bounds.\n",
    "\n",
    "    Args:\n",
    "        ttd (pd.Series): Time to death values.\n",
    "        y_values (pd.Series): Initial label values.\n",
    "        lowerbound (int): Lower day bound.\n",
    "        upperbound (int): Upper day bound.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Updated binary labels.\n",
    "    \"\"\"\n",
    "    for index in range(len(ttd)):\n",
    "        if(lowerbound<=ttd[index]<=upperbound):\n",
    "            y_values[index] = 1\n",
    "        else:\n",
    "            y_values[index] = 0\n",
    "    return y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33871e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "def winsorize_col(column):\n",
    "    \"\"\"\n",
    "    Winsorizes a pandas Series to limit extreme values.\n",
    "\n",
    "    Args:\n",
    "        column (pd.Series): Input data column.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Winsorized column.\n",
    "    \"\"\"\n",
    "    return winsorize(column, limits=(0.001, .001))\n",
    "\n",
    "def winsorize_df(df):\n",
    "    \"\"\"\n",
    "    Applies winsorization to all columns in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Winsorized DataFrame.\n",
    "    \"\"\"\n",
    "    return df.apply(winsorize_col,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96072373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(df,day):\n",
    "    \"\"\"\n",
    "    Prepares the DataFrame for modeling by formatting labels, dropping unnecessary columns,\n",
    "    removing duplicates and NaNs, and winsorizing features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        day (tuple): Day bounds for label formatting.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (features DataFrame, fold assignments, labels)\n",
    "    \"\"\"\n",
    "    ttd = df['ttd']\n",
    "    y_arr = df['y']\n",
    "    y_arr = format_y(ttd,y_arr,day[0],day[1])\n",
    "    df['y'] = y_arr\n",
    "    # change the y column in the df to the y_arr\n",
    "\n",
    "    # we only care if the trainign data colun values contain NaN values \n",
    "    df_drop_col = df.drop(['VitalID','ttd','time'], axis=1)\n",
    "    df_drop_col_dup = df_drop_col.drop_duplicates()\n",
    "    df_drop_col_dup_na = df_drop_col_dup.dropna()\n",
    "\n",
    "    y_arr = df_drop_col_dup_na['y']\n",
    "    fold_arr = df_drop_col_dup_na['fold']\n",
    "\n",
    "    df_drop_col_dup_na.drop(['y','fold'], axis=1,inplace=True)\n",
    "    df = winsorize_df(df_drop_col_dup_na)\n",
    "    return df,fold_arr,y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f693e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Basics = [ \"VitalID\",\"time\",\"fold\",\"ttd\",\"y\"]\n",
    "\n",
    "Demographics = ['bwt', 'ega', 'male', 'apgar5']\n",
    "\n",
    "HR = ['HR.SB.MotifTwo.diff.uu', 'HR.SB.MotifTwo.diff.uuu','HR.ST.LocalExtrema.l25.diffmaxabsmin', 'HR.std','HR.SB.MotifThree.diffquant.hhhh',\n",
    "       'HR.EX.MovingThreshold.a0.25.b0.1.meanqover', 'HR.DN.cv.3','HR.SB.MotifTwo.iqr.ddd','HR.SY.StdNthDer.5','HR.ST.LocalExtrema.l25.maxmaxmed', 'HR.SY.StdNthDer.17', 'HR.Quantile.99', 'HR.FC.Suprise.tstat',\n",
    "       'HR.SB.MotifThree.quantile.hhhh','HR.EX.MovingThreshold.a0.25.b0.05.meanqover','HR.PH.Walkerrunningvar...sw.meanabsdiff', 'HR.CO.tc3.1..denom', 'HR.mean',\n",
    "       'HR.skew2','HR.SB.TransitionMatrix23.sumdiagcov','HR.ST.LocalExtrema.n100.minabsmin', 'HR.kurt2', 'HR.MF.arfit.sbc.7'\n",
    "       ]\n",
    "\n",
    "SPO2 = [ 'SP.EX.MovingThreshold.a0.25.b0.1.meanqover', 'SP.EX.MovingThreshold.a1.b0.25.iqrq', 'SP.PH.Walkerbiasprop.0.1..0.5..sw.meanabsdiff',\n",
    "'SP.EX.MovingThreshold.a0.25.b0.1.maxq', 'SP.PH.Walkerprop.0.9..w.std', 'SP.skew2','SP.SB.MotifTwo.diff.dduu','SP.DN.RemovePointsmin.0.2.mean',\n",
    "'SP.PH.Walkermomentum.2..sw.stdrat','SP.CO.tc3.1..denom','SP.mean','SP.kurt2', 'SP.SB.MotifThree.diffquant.hhh','SP.SB.MotifTwo.mean.dddd',\n",
    "'SP.AutoCorr.lag.4','SP.SB.TransitionMatrix41.ondiag', 'SP.SB.MotifThree.quantile.baaa','SP.std', 'SP.SB.TransitionMatrix21.T10', 'SP.SB.BinaryMethod.iqr.pstretch1'\n",
    ",'SP.PH.Walkerprop.0.9..sw.stdrat','SP.MF.arfit.sbc.7','SP.SB.TransitionMatrix22.mineig','SP.ST.LocalExtrema.n100.minabsmin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a6b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diff_dfs(df,df_dict):\n",
    "    \"\"\"\n",
    "    Creates different feature DataFrames based on specified feature groups.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        df_dict (dict): Mapping of index to feature group name.\n",
    "\n",
    "    Returns:\n",
    "        list: List of DataFrames for each feature group.\n",
    "    \"\"\"\n",
    "    df_hr = df[Basics + HR]\n",
    "    df_spo2 = df[Basics + SPO2]\n",
    "    df_demographics = df[Basics + Demographics]\n",
    "    df_hr_spo2 = df[Basics + HR + SPO2] \n",
    "    df_hr_spo2_demographics = df[Basics + HR + SPO2 + Demographics]\n",
    "    data_dict = {'df_hr': df_hr,'df_spo2':df_spo2,'df_demographics':df_demographics,'df_hr_spo2':df_hr_spo2,'df_hr_spo2_demographics':df_hr_spo2_demographics}\n",
    "    \n",
    "    df_arr = []\n",
    "    for i in df_dict:\n",
    "        df_arr.append(0)\n",
    "    \n",
    "    for index in df_dict:\n",
    "        df_val = df_dict[index]\n",
    "        df_arr[index] = data_dict[df_val]\n",
    "    return df_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d442ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [df_hr,df_spo2,df_demographics,df_hr_spo2,df_hr_spo2_demographics] = create_diff_dfs(df)\n",
    "\n",
    "# for i in [df_hr,df_spo2,df_demographics,df_hr_spo2,df_hr_spo2_demographics]:\n",
    "#     print(i.groupby(['fold', 'y']).agg(Count=(\"VitalID\",\"count\")).reset_index())\n",
    "#     print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0434fac1",
   "metadata": {},
   "source": [
    "## Model Training and Generating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b1cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logits_arr(no_of_logits):\n",
    "    \"\"\"\n",
    "    Generates an array of threshold logits for classification.\n",
    "\n",
    "    Args:\n",
    "        no_of_logits (int): Number of logits to generate.\n",
    "\n",
    "    Returns:\n",
    "        list: Sorted list of logits.\n",
    "    \"\"\"\n",
    "    # base_logit = 0.1/int(no_of_logits/2)\n",
    "    # other_logit = 1/int(no_of_logits/2)\n",
    "    # logits_arr_base = ([base_logit*i for i in range(1,int(no_of_logits/2)+1)])[::-1]\n",
    "    # logits_arr_other = ([other_logit*i for i in range(1,int(no_of_logits/2)+1)])[::-1]\n",
    "    # for logit in logits_arr_base:\n",
    "    #     logits_arr_other.append(logit)\n",
    "    # logits_arr = sorted(logits_arr_other)\n",
    "    \n",
    "    x = np.linspace(0, 1, no_of_logits)\n",
    "    k = 2\n",
    "    logits_arr = (np.exp(k*x) - 1) / (np.exp(k) - 1)\n",
    "    return logits_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_arr = generate_logits_arr(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a815763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_pred,y_true):\n",
    "    \"\"\"\n",
    "    Computes recall, precision, and false positive rate.\n",
    "\n",
    "    Args:\n",
    "        y_pred (np.ndarray): Predicted binary labels.\n",
    "        y_true (np.ndarray): True binary labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (recall, precision, false positive rate)\n",
    "    \"\"\"\n",
    "    true_positives = sum(np.logical_and(y_pred==1,y_true==1))\n",
    "    false_negatives = sum(np.logical_and(y_pred == 0, y_true==1))\n",
    "    false_positives = sum(np.logical_and(y_pred == 1, y_true==0))\n",
    "    true_negatives = sum(np.logical_and(y_pred==0,y_true==0))\n",
    "\n",
    "    recall = true_positives/(true_positives+false_negatives)\n",
    "    precision = true_positives/(true_positives+false_positives)\n",
    "    fpr = false_positives/(false_positives+true_negatives)\n",
    "    \n",
    "    return recall,precision,fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(results,recalls_arr,precisions_arr,false_positive_rates_arr,roc_auc_scores,auprc_scores,cv_scores,curr_features):\n",
    "    \"\"\"\n",
    "    Stores evaluation metrics and feature information in the results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Dictionary to store results.\n",
    "        recalls_arr (list): Recall values for each fold.\n",
    "        precisions_arr (list): Precision values for each fold.\n",
    "        false_positive_rates_arr (list): FPR values for each fold.\n",
    "        roc_auc_scores (list): ROC-AUC scores for each fold.\n",
    "        auprc_scores (list): AUPRC scores for each fold.\n",
    "        cv_scores (list): Cross-validation accuracy scores.\n",
    "        curr_features (list): Current feature names.\n",
    "    \"\"\"\n",
    "    # Store results\n",
    "    results['num_features'].append(len(curr_features))\n",
    "    results['cv_score_mean'].append(np.mean(cv_scores))\n",
    "    results['cv_score_std'].append(np.std(cv_scores))\n",
    "    results['roc_auc_score'].append(np.mean(roc_auc_scores))\n",
    "    results['aurpc_score'].append(np.mean(auprc_scores))\n",
    "\n",
    "    recalls_arr_mean = np.mean(recalls_arr,axis =0)  # true postive rate = recall\n",
    "    precisions_arr_mean = np.mean(precisions_arr,axis =0)\n",
    "    false_positive_rates_arr_mean = np.mean(false_positive_rates_arr,axis=0)\n",
    "    recall_arr_std = np.std(recalls_arr,axis=0)\n",
    "    precision_arr_std = np.std(precisions_arr,axis=0)\n",
    "    false_positive_rate_arr_std = np.std(false_positive_rates_arr,axis=0)\n",
    "\n",
    "    results['recalls_arr_mean'].append(recalls_arr_mean)\n",
    "    results['precisions_arr_mean'].append(precisions_arr_mean)\n",
    "    results['false_positive_rates_arr_mean'].append(false_positive_rates_arr_mean)\n",
    "    results['recall_arr_std'].append(recall_arr_std)\n",
    "    results['precision_arr_std'].append(precision_arr_std)\n",
    "    results['false_positive_rate_arr_std'].append(false_positive_rate_arr_std)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492cb871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_features(feature_importances,curr_features,decrement,lower_bound):\n",
    "    \"\"\"\n",
    "    Removes least important features based on permutation importance.\n",
    "\n",
    "    Args:\n",
    "        feature_importances (list): Feature importances from each fold.\n",
    "        curr_features (list): Current feature names.\n",
    "        decrement (int): Number of features to remove.\n",
    "        lower_bound (int): Minimum number of features to retain.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if features were updated, False if lower bound reached.\n",
    "    \"\"\"\n",
    "    orig_curr_features = curr_features\n",
    "    # Average feature importance across folds\n",
    "    avg_importance = np.mean(feature_importances, axis=0)\n",
    "\n",
    "    # Find least important features\n",
    "    num_to_remove = min(decrement, len(curr_features) - lower_bound)\n",
    "    \n",
    "    if num_to_remove == 0:\n",
    "        return False\n",
    "\n",
    "    remove_idx = np.argsort(avg_importance)[:num_to_remove]\n",
    "    remove_features = [curr_features[i] for i in remove_idx]\n",
    "\n",
    "    # Remove features for next iteration\n",
    "    # works on feature names....have to ensure feature names are different\n",
    "    curr_features = [f for f in curr_features if f not in remove_features]\n",
    "    for i in range(len(orig_curr_features)):\n",
    "        orig_curr_features.pop()\n",
    "    \n",
    "    for feature in curr_features:\n",
    "        orig_curr_features.append(feature) \n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e9ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metrics_for_logits(logits_arr,y_scores,y_val):\n",
    "    \"\"\"\n",
    "    Computes recall, precision, and FPR for each logit threshold.\n",
    "\n",
    "    Args:\n",
    "        logits_arr (list): List of threshold logits.\n",
    "        y_scores (np.ndarray): Predicted probabilities.\n",
    "        y_val (np.ndarray): True labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (recall_arr, precision_arr, false_positive_rate_arr)\n",
    "    \"\"\"\n",
    "    recall_arr = []\n",
    "    precision_arr = []\n",
    "    false_positive_rate_arr = []\n",
    "    y_val = np.array(y_val)\n",
    "\n",
    "    for logit in logits_arr:\n",
    "\n",
    "        y_scores_pred = np.array(y_scores) > logit\n",
    "        recall,precision,fpr = compute_metrics(y_scores_pred,y_val)\n",
    "\n",
    "        if(math.isnan(precision)):\n",
    "            precision_arr.append(1)\n",
    "        else:\n",
    "            precision_arr.append(precision)\n",
    "\n",
    "        recall_arr.append(recall)\n",
    "        false_positive_rate_arr.append(fpr)\n",
    "    \n",
    "    return recall_arr,precision_arr,false_positive_rate_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_results(df,fold_arr: list,y_arr: list,decrement: int,lower_bound: int,max_iters: int,logits_arr: list):\n",
    "    \"\"\"\n",
    "    Performs iterative feature elimination and cross-validation, storing metrics at each step.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Feature DataFrame.\n",
    "        fold_arr (pd.Series): Fold assignments.\n",
    "        y_arr (pd.Series): Target labels.\n",
    "        decrement (int): Number of features to remove per iteration.\n",
    "        lower_bound (int): Minimum number of features to retain.\n",
    "        max_iters (int): Maximum iterations for model fitting.\n",
    "        logits_arr (list): Threshold logits for metrics.\n",
    "\n",
    "    Returns:\n",
    "        dict: Results dictionary containing metrics and feature info.\n",
    "    \"\"\"\n",
    "    results = defaultdict(list)\n",
    "    folds = sorted(fold_arr.unique())\n",
    "    curr_features = list(df.columns)\n",
    "\n",
    "    while len(curr_features) >= lower_bound:\n",
    "        cv_scores = []\n",
    "        feature_importances = []\n",
    "        roc_auc_scores = []\n",
    "        auprc_scores = []\n",
    "\n",
    "        precisions_arr = []\n",
    "        recalls_arr = []\n",
    "        false_positive_rates_arr = []\n",
    "        y_vals_arr = []\n",
    "        y_scores_arr = []\n",
    "\n",
    "        print(\"Current features: \" + str(len(curr_features)))\n",
    "\n",
    "        for fold_val in folds:\n",
    "\n",
    "            # Split data\n",
    "            X_val = df[fold_arr == fold_val][curr_features].to_numpy()\n",
    "            y_val = y_arr[fold_arr == fold_val].to_numpy()\n",
    "\n",
    "            X_train = df[fold_arr != fold_val][curr_features].to_numpy()\n",
    "            y_train = y_arr[fold_arr != fold_val].to_numpy()\n",
    "\n",
    "            # Fit model\n",
    "            model = LogisticRegression(max_iter=max_iters,penalty='l2').fit(X_train, y_train)\n",
    "\n",
    "            # get the test out set scores\n",
    "            y_pred = model.predict(X_val)\n",
    "            acc = accuracy_score(y_val, y_pred)\n",
    "            cv_scores.append(acc)\n",
    "            \n",
    "            # Get the probability estimates for the positive class\n",
    "            y_scores = model.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "            y_vals_arr.append(y_val)\n",
    "            y_scores_arr.append(y_scores)\n",
    "\n",
    "            # Permutation importance\n",
    "            perm = permutation_importance(model, X_val, y_val, n_repeats=10, random_state=42)\n",
    "            feature_importances.append(perm.importances_mean)\n",
    "            roc_auc_scores.append(roc_auc_score(y_val,y_scores))\n",
    "            auprc_scores.append(average_precision_score(y_val, y_scores))\n",
    "            \n",
    "            recall_arr,precision_arr,false_positive_rate_arr = update_metrics_for_logits(logits_arr,y_scores,y_val)\n",
    "            recalls_arr.append(recall_arr)\n",
    "            \n",
    "            precisions_arr.append(precision_arr)\n",
    "            false_positive_rates_arr.append(false_positive_rate_arr)\n",
    "\n",
    "        # storing the results in the results_dic\n",
    "        store_results(results,recalls_arr,precisions_arr,false_positive_rates_arr,roc_auc_scores,auprc_scores,cv_scores,curr_features)\n",
    "        results['y_val'].append(y_vals_arr)\n",
    "        results['y_scores'].append(y_scores_arr)\n",
    "        \n",
    "        # update the curr_features arr\n",
    "        if (not(update_features(feature_importances,curr_features,decrement,lower_bound))):\n",
    "            break\n",
    "\n",
    "    # Convert results to DataFrame for inspection\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3fc993",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = OrderedSet(( (0,1), (0,3), (0,7), (3,4), (7,8)))\n",
    "dfs_dict = { 0: 'df_hr_spo2' , 1: 'df_hr_spo2_demographics' }\n",
    "# days = OrderedSet( ((0,1), ))\n",
    "# dfs_dict = { 0: 'df_hr_spo2' }\n",
    "dfs_arr = create_diff_dfs(df,dfs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28962465",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = defaultdict(list)\n",
    "no_of_model_features = [5,6] # specifying the lower bound\n",
    "decrement = 5  # specifying the no of features to decrement by every time\n",
    "\n",
    "for day in days: \n",
    "    print(f\"Day: {day}\")\n",
    "    for dataframe_index in range(len(dfs_arr)):\n",
    "        print(f\"{dfs_dict[dataframe_index]}\")\n",
    "        data,fold_arr,y_arr = create_df(dfs_arr[dataframe_index],day)\n",
    "        results = give_results(data,fold_arr,y_arr,decrement,no_of_model_features[dataframe_index],100,logits_arr)\n",
    "        results_dict[day].append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3758a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_vals_arr = results_dict[(0,1)][0]['y_val'][0]\n",
    "# y_scores_arr = results_dict[(0,1)][0]['y_scores'][0]\n",
    "\n",
    "# for i in range(len(y_vals_arr)):\n",
    "#    # PrecisionRecallDisplay.from_predictions(y_vals_arr[i],y_scores_arr[i])\n",
    "   \n",
    "#    fpr, tpr, _ = roc_curve(y_vals_arr[i], y_scores_arr[i])\n",
    "#    roc_auc = auc(fpr, tpr)\n",
    "#    plt.plot(fpr, tpr, label=f'Fold {i+1} (AUC = {roc_auc:.2f})')\n",
    "#    # precision,recall,_ = precision_recall_curve(np.array(y_vals_arr[i]),np.array(y_scores_arr[i]))\n",
    "#    # plot_precision_recall_curve()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef417f4",
   "metadata": {},
   "source": [
    "## Storing and the plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d264eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results5.pkl', 'wb') as f:\n",
    "        # Load the data from the pickle file\n",
    "        pl.dump(results_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85827a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results5.pkl', 'rb') as f:\n",
    "        # Load the data from the pickle file\n",
    "        loaded_data = pl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e58081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_plots(results_dir,loaded_data):\n",
    "    \"\"\"\n",
    "    Saves plots and tables of results for each day and feature group.\n",
    "\n",
    "    Args:\n",
    "        results_dir (str): Directory to save results.\n",
    "        loaded_data (dict): Results data loaded from pickle.\n",
    "    \"\"\"\n",
    "    output_dir = results_dir\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    for day in days: \n",
    "        # create dir for day\n",
    "        \n",
    "        output_dir = results_dir\n",
    "        filename = f\"Day: {day}\"\n",
    "        day_filepath = os.path.join(output_dir, filename)\n",
    "        if not os.path.isdir(day_filepath):\n",
    "            os.makedirs(day_filepath)\n",
    "        \n",
    "        \n",
    "        for dataframe_index in range(len(dfs_arr)):\n",
    "            \n",
    "            # creating the plots\n",
    "            \n",
    "            # create dir for dataframe_index..this is the last dir\n",
    "            output_dir =  day_filepath\n",
    "            filename = f\"{dfs_dict[dataframe_index]}\"\n",
    "            df_filepath = os.path.join(output_dir, filename)\n",
    "            if not os.path.isdir(df_filepath):\n",
    "                os.makedirs(df_filepath)\n",
    "            \n",
    "            \n",
    "            result = loaded_data[day][dataframe_index]\n",
    "\n",
    "            x_precisions = np.array(result['precisions_arr_mean'])\n",
    "            x_recalls = np.array(result['recalls_arr_mean'])\n",
    "            x_fprs = np.array(result['false_positive_rates_arr_mean'])\n",
    "            \n",
    "            x_precision_stds = np.array(result['precision_arr_std'])\n",
    "            x_recall_stds = np.array(result['recall_arr_std'])\n",
    "            x_fpr_stds = np.array(result['false_positive_rate_arr_std'])\n",
    "            \n",
    "            roc_auc_scores = result['roc_auc_score']\n",
    "            aurpc_scores = result['aurpc_score']\n",
    "            num_features = result['num_features']\n",
    "            \n",
    "            \n",
    "            fig, axes = plt.subplots(nrows=1, ncols=len(x_precisions), figsize=(5 * len(x_precisions), 5),squeeze=False)\n",
    "            \n",
    "            for plot_index in range(len(x_precisions)):\n",
    "                \n",
    "                ax=axes[0][plot_index]\n",
    "                \n",
    "                x_precision = x_precisions[plot_index]\n",
    "                x_recall = x_recalls[plot_index]\n",
    "                \n",
    "                x_precision_std = x_precision_stds[plot_index]\n",
    "                x_recall_std = x_recall_stds[plot_index]\n",
    "            \n",
    "                aurpc_score = aurpc_scores[plot_index]\n",
    "                \n",
    "                lower = np.maximum(x_precision - x_precision_std, 0.0).reshape(-1)\n",
    "                upper = np.minimum(x_precision + x_precision_std, 1.0).reshape(-1)\n",
    "                ax.plot(x_recall,x_precision, color=\"blue\")\n",
    "                ax.set_title(\"Precision Recall curve\")\n",
    "                ax.set_xlim(0,1)\n",
    "                ax.set_ylim(0,1)\n",
    "                ax.fill_between(x_recall.reshape(-1), lower, upper, alpha=0.3,color='0.8')\n",
    "                ax.text(0.5, 0.5, f'Auprc score: {int(aurpc_score*1000)/1000}', fontsize=12, color='red')\n",
    "                ax.text(0.5, 0.4, f'No of features: {num_features[plot_index]}', fontsize=12, color='red')\n",
    "                ax.set_xlabel(\"Recall\")\n",
    "                ax.set_ylabel(\"Precision\")\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{df_filepath}/PR curve.png\", dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            \n",
    "            \n",
    "            fig, axes = plt.subplots(nrows=1, ncols=len(x_fprs), figsize=(5 * len(x_fprs), 5),squeeze=False)\n",
    "            \n",
    "            for plot_index in range(len(x_fprs)):\n",
    "                \n",
    "                ax=axes[0][plot_index]\n",
    "                \n",
    "                x_recall = x_recalls[plot_index]\n",
    "                x_fpr = x_fprs[plot_index]\n",
    "            \n",
    "                x_recall_std = x_recall_stds[plot_index]\n",
    "                roc_auc_score = roc_auc_scores[plot_index]\n",
    "                \n",
    "                lower = np.maximum(x_recall - x_recall_std, 0.0).reshape(-1)\n",
    "                upper = np.minimum(x_recall + x_recall_std, 1.0).reshape(-1)\n",
    "                ax.plot(x_fpr,x_recall, color=\"blue\")\n",
    "                ax.set_title(\"ROC-AUC curve\")\n",
    "                ax.set_xlim(0,1)\n",
    "                ax.set_ylim(0,1)\n",
    "                ax.fill_between(x_fpr.reshape(-1), lower, upper, alpha=0.3,color='0.8')\n",
    "                ax.text(0.5, 0.5, f'AUC-ROC score: {int(roc_auc_score*1000)/1000}', fontsize=12, color='red')\n",
    "                ax.text(0.5, 0.4,  f'No of features: {num_features[plot_index]}', fontsize=12, color='red')\n",
    "                ax.set_xlabel(\"FPR\")\n",
    "                ax.set_ylabel(\"Recall\")\n",
    "\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{df_filepath}/ROC-AUC curve.png\", dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=len(x_precisions), figsize=(5 * len(x_precisions), 5),squeeze=False)\n",
    "\n",
    "            columns = ['recall','precision']\n",
    "            for plot_index in range(len(x_precisions)):\n",
    "                \n",
    "                ax=axes[0][plot_index]\n",
    "                ax.axis('off')\n",
    "                \n",
    "                values = [0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.99]\n",
    "                data = []\n",
    "                \n",
    "                for value in values:\n",
    "                    recall_values = abs(x_recalls[plot_index] - value)\n",
    "                    index = np.argmin(recall_values)\n",
    "                    recall = recall_values[index]\n",
    "                    precision =  x_precisions[plot_index][index]\n",
    "                    data.append([value,precision])\n",
    "            \n",
    "                    \n",
    "                table = ax.table(cellText=data, colLabels=columns, loc='center')\n",
    "                ax.text(0.5, 0.07,  f'No of features: {num_features[plot_index]}', fontsize=12, color='red')\n",
    "                ax.text(0.5, 0.14,  f'AUC-ROC score: {int(aurpc_scores[plot_index]*1000)/1000}', fontsize=12, color='red')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{df_filepath}/tables.png\", dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b4d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_plots('/Users/adityagoyal/Desktop/Research - yin li/baseline/results5',loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision is basically out of all the datapoints you predicted as a class, how much of that is correct.\n",
    "# Recall is basically, out of all the datapoints belonging to that class, how much were you able to predict correctly.\n",
    "# You can get this intuition for either of the classes, especially for the imbalanced class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08106467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive and negative are the two classes\n",
    "# The true positive rate is a measure of the probability that an actual positive instance will be classified as positive.\n",
    "# The true negative rate is a measure of the probability that an actual negative instance will be classified as negative. \n",
    "# he false positive rate is essentially a measure of how often a \"false alarm\" will occur – or, how often an actual negative instance will be classified as positive. (actually this metric says inofrmation aboput the other class!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b1134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision = tp/(tp+fp)\n",
    "# recall = tp/(tp + fn)  (also known as the true positive rate)\n",
    "# f1 = 2*precision*recall/ (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we talk about metrics we talk about only pertaining to one class\n",
    "# true positive rate = tp/(tp + fn) \n",
    "# false positive rate = fp(falsely predicted negative datapoints)/fp+tn (all the negative datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eac931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your use case determines how much importance you give to partiuclar evaluation metrics and which evluation metrics you use and on which class - the 0 or the 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8951919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each fold(hold out set): \n",
    "    # Get the predicted logits\n",
    "    # figure out the different values of the boundary logits (from 0 to 1) say k (like randomly generate unformly spaced values)\n",
    "    # get the precision recall values for each of the boundary logits \n",
    "\n",
    "# average out the precision scores at a particualr boundary logit across the various folds \n",
    "# average out the recall scores at a particualr boundary logit across the various folds \n",
    "# take the stadnard deviation of the precision socres at a particualr boundary logit across the various folds \n",
    "# take the stadnard deviation of the recall socres at a particualr boundary logit across the various folds \n",
    "\n",
    "# shape of the precision scores: (folds,k)\n",
    "# shape of the recall curves: (folds,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each fold(hold out set): \n",
    "    # Get the predicted logits\n",
    "    # figure out the different values of the boundary logits (from 0 to 1) say k (like randomly generate unformly spaced values)\n",
    "    # get the tpr and fpr values for each of the boundary logits \n",
    "\n",
    "# average out the tpr socres at a particualr boundary logit across the various folds \n",
    "# average out the fpr scores at a particualr boundary logit across the various folds \n",
    "# take the stadnard deviation of the tpr socres at a particualr boundary logit across the various folds \n",
    "# take the stadnard deviation of the fpr socres at a particualr boundary logit across the various folds \n",
    "\n",
    "# shape of the tpr scores: (folds,k)\n",
    "# shape of the fpr curves: (folds,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07077755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while (curr_no_of_features(variable) >= 5)\n",
    "    # for the different no of folds(variable)\n",
    "        # take only that fold dataset from the df(variable) and its corresponding y for validation \n",
    "        # the rest of the data will be used to train the model\n",
    "\n",
    "        # fit the model \n",
    "        # do the inference on the hold out set \n",
    "        # store the cv accuracy score \n",
    "\n",
    "        # call the permuation_importance function and provide the fitted model and hold out set as arguments \n",
    "        # store the feature importance of all the features \n",
    "\n",
    "    # average out the feature importance across the folds and get the argmin \n",
    "    # store the no of features \n",
    "    # store the average of the cv scores\n",
    "    # store the standard deviation of the cv scores\n",
    "\n",
    "    # no_of_features to decrease = min(5,curr_no_features-5)\n",
    "    # if(no_of_features to decrease):\n",
    "    #      break\n",
    "    # remove the features from the dataset and continue with the changed dataset \n",
    "\n",
    "# Arguments: df(the dataset features), fold_arr, y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b013fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different feature elimination methods \n",
    "# do different regularization methods \n",
    "# see if NaN values should be removed \n",
    "# solve the issue of roc-auc curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10538c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model architecture \n",
    "# read the rnn paper again...tell abrar the entire experimental setup.\n",
    "# # you have to capture as mich essence as possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
